# ğŸ“Š YouTube Trending Data Pipeline with Airflow, Docker & Python

This project builds an end-to-end **ETL data pipeline** that fetches real-time trending video data from the **YouTube Data API**, cleans the data, and loads it into a **SQLite database** using **Apache Airflow**. The entire workflow is containerized using **Docker** for portability and reproducibility.

---

## ğŸš€ Project Overview

### âœ… What It Does:
- ğŸ“¥ **Extract:** Pulls trending video data via YouTube API (for India region)
- ğŸ§¹ **Transform:** Cleans and structures the raw JSON into a tabular CSV
- ğŸ—ƒï¸ **Load:** Saves cleaned data into a SQLite database for querying
- ğŸ› ï¸ **Orchestrates:** All steps automated using Apache Airflow DAGs
- ğŸ“¦ **Runs in Docker:** Fully containerized for easy setup and deployment

---

## ğŸ› ï¸ Tech Stack

| Tool            | Purpose                             |
|-----------------|-------------------------------------|
| Python          | ETL scripting                       |
| Pandas          | Data manipulation                   |
| SQLite          | Lightweight relational DB           |
| Apache Airflow  | Workflow orchestration              |
| YouTube Data API| Data source                         |
| Docker          | Containerized environment           |

---

## ğŸ“ Folder Structure

<pre><code> docker/ â”œâ”€â”€ airflow/ â”‚ â”œâ”€â”€ dags/ â”‚ â”‚ â””â”€â”€ youtube_dag.py # Airflow DAG definition â”‚ â”œâ”€â”€ scripts/ â”‚ â”‚ â”œâ”€â”€ extract.py # YouTube API data extractor â”‚ â”‚ â”œâ”€â”€ transform.py # Data cleaning script â”‚ â”‚ â””â”€â”€ load.py # Load data into SQLite â”‚ â”œâ”€â”€ data/ â”‚ â”‚ â”œâ”€â”€ raw_data.json # Raw YouTube API output â”‚ â”‚ â”œâ”€â”€ clean_data.csv # Cleaned CSV data â”‚ â”‚ â””â”€â”€ youtube_trending.db # Final SQLite database â”‚ â””â”€â”€ airflow.cfg (optional) # Airflow config (ignored in .gitignore) â”‚ â”œâ”€â”€ docker-compose.yml # Docker orchestration file â”œâ”€â”€ Dockerfile # Custom Airflow Docker image â”œâ”€â”€ README.md # Project documentation â””â”€â”€ .gitignore # Prevents heavy/log files from uploading </code></pre>

## âš™ï¸ How to Run the Project

### ğŸ”§ Prerequisites
- Docker & Docker Compose installed
- Valid YouTube Data API key (free from [Google Cloud Console](https://console.cloud.google.com/))

Insert your API key into extract.py:
API_KEY = "YOUR_API_KEY"

Start Airflow:
docker-compose up -d --build

Open Airflow UI:

Visit: http://localhost:8080

Login: admin / (password auto-generated in logs)

Trigger the DAG:

Turn on the DAG named youtube_trending_pipeline
Click â–¶ï¸ Trigger button

Check Outputs:

Visit docker/airflow/data/ to see:

raw_data.json
clean_data.csv
youtube_trending.db

